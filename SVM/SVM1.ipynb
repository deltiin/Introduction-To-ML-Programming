{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltiin/Introduction-To-ML-Programming/blob/main/SVM/SVM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOx67gis0unk"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_kuNicPwmnJ"
      },
      "source": [
        "Our objective is to find a hyperplane that separates +ve and -ve examples with the largest margin while keeping the misclassification as low as possible \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB_7Yp0Ly2X9"
      },
      "source": [
        "To separate the two classes of data points, there are many possible hyperplanes. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHC2lc9mzSlM"
      },
      "source": [
        "Using the Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrCQ9SIxzlAa"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgmhT8qx7mJn"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znKs_HL8zA2l"
      },
      "source": [
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
        "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
        "df = read_csv(url, names=names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpG0dbjhzueg"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEly61sv33Ij"
      },
      "source": [
        "Reduce the dataset to 100 rows (to show that SVM works on a small dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsw2zVlzSwc"
      },
      "source": [
        "target = df['class']\n",
        "s = set()\n",
        "for val in target:\n",
        "    s.add(val)\n",
        "s = list(s)\n",
        "rows = list(range(100,150))\n",
        "df = df.drop(df.index[rows])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC12EDO01ZYX"
      },
      "source": [
        "Remove one of the classes to make this a binary class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_9jwtsh0vuP"
      },
      "source": [
        "x = df['sepal-length']\n",
        "y = df['petal-length']\n",
        "\n",
        "setosa_x = x[:50]\n",
        "setosa_y = y[:50]\n",
        "\n",
        "versicolor_x = x[50:]\n",
        "versicolor_y = y[50:]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(setosa_x,setosa_y,marker='+',color='green')\n",
        "plt.scatter(versicolor_x,versicolor_y,marker='o',color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mQBHKpA1i6l"
      },
      "source": [
        "Split into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USFiKcON03_L"
      },
      "source": [
        "## Drop rest of the features and extract the target values\n",
        "#df = df.drop(['sepal-length','petal-length'],axis=1)\n",
        "Y = []\n",
        "target = df['class']\n",
        "for val in target:\n",
        "    if(val == 'Iris-setosa'):\n",
        "        Y.append(-1)\n",
        "    else:\n",
        "        Y.append(1)\n",
        "df = df.drop(['class'],axis=1)\n",
        "X = df.values.tolist()\n",
        "## Shuffle and split the data into training and test set\n",
        "X, Y = shuffle(X,Y)\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.9)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = y_train.reshape(90,1)\n",
        "y_test = y_test.reshape(10,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMsRueDM_EUZ"
      },
      "source": [
        "#x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-aydAK-4eo9"
      },
      "source": [
        "Create the SVM model using the sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0WEZAsq2RxF"
      },
      "source": [
        "clf = SVC(kernel='linear')\n",
        "clf.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7LVQHs56Nch"
      },
      "source": [
        "Get the accuracy of the SVM model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eDqIksf6MTz"
      },
      "source": [
        "y_pred = clf.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxl-D5E41y43"
      },
      "source": [
        "# **Create the SVM model from scratch**<br>\n",
        "\n",
        "α(0.0001) is the learning rate and the regularization parameter λ is set to 1/epochs. Therefore, the regularizing value deceases as the number of epochs increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yS3f7Fz2BJ8"
      },
      "source": [
        "## Support Vector Machine \n",
        "train_f1 = x_train[:,0]\n",
        "train_f2 = x_train[:,1]\n",
        "\n",
        "train_f1 = train_f1.reshape(90,1)\n",
        "train_f2 = train_f2.reshape(90,1)\n",
        "\n",
        "w1 = np.zeros((90,1))\n",
        "w2 = np.zeros((90,1))\n",
        "\n",
        "epochs = 1\n",
        "alpha = 0.0001\n",
        "\n",
        "while(epochs < 10000):\n",
        "    y = w1 * train_f1 + w2 * train_f2\n",
        "    prod = y * y_train\n",
        "    #print(epochs)\n",
        "    count = 0\n",
        "    for val in prod:\n",
        "        if(val >= 1):\n",
        "            cost = 0\n",
        "            w1 = w1 - alpha * (2 * 1/epochs * w1)\n",
        "            w2 = w2 - alpha * (2 * 1/epochs * w2)\n",
        "            \n",
        "        else:\n",
        "            cost = 1 - val \n",
        "            w1 = w1 + alpha * (train_f1[count] * y_train[count] - 2 * 1/epochs * w1)\n",
        "            w2 = w2 + alpha * (train_f2[count] * y_train[count] - 2 * 1/epochs * w2)\n",
        "        count += 1\n",
        "    epochs += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuqBNx012JMs"
      },
      "source": [
        "Extract the features from the test data and predict the values. We obtain the predictions and compare it with the actual values and print the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYBWswOb2KBf"
      },
      "source": [
        "## Use only the first 10 weights to match the 10 test data points\n",
        "index = list(range(10,90))\n",
        "w1 = np.delete(w1,index)\n",
        "w2 = np.delete(w2,index)\n",
        "\n",
        "w1 = w1.reshape(10,1)\n",
        "w2 = w2.reshape(10,1)\n",
        "## Extract the test data features \n",
        "test_f1 = x_test[:,0]\n",
        "test_f2 = x_test[:,1]\n",
        "\n",
        "test_f1 = test_f1.reshape(10,1)\n",
        "test_f2 = test_f2.reshape(10,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MnmhWvU6blr"
      },
      "source": [
        "Determine the accuracy of the scratch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DufyB6pf3C5R"
      },
      "source": [
        "## Predict\n",
        "y_pred = w1 * test_f1 + w2 * test_f2\n",
        "predictions = []\n",
        "for val in y_pred:\n",
        "    if(val > 1):\n",
        "        predictions.append(1)\n",
        "    else:\n",
        "        predictions.append(-1)\n",
        "\n",
        "print(accuracy_score(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMzAB7vH0d-a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}